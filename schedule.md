---
layout: page
title: Schedule
---

# Timetable

||Monday 26th|Tuesday 27th|Wednesday 28th|
|9:00 - 10:00|Wang|Melko|Melko|
|10:00 - 11:30|Wang|Kim|Ringel|
|11:30 - 12:00|*Coffee break*|*Coffee break*|*Coffee break*|
|12:00 - 13:30|Kim|Wang|Ringel|
|13:30 - 15:00|*Lunch*|*Lunch*|*Lunch*|
|15:00 - 16:00|Wang|Melko|Melko|
|16:00 - 16:30|*Coffee break*|*Coffee break*|*Coffee break*|
|16:30 - 17:30|Melko|Kim|Ringel|
|17:30 - 18:30|Kim|Ringel|Wrap up session|


# Materials covered

## Lei Wang

Deep learning theory, Generative models for physicists, Differentiable programming, Representation Learning. Examples to be covered: computation graph, automatic differentiation, variational inference, inverse Hamiltonian design. Applications of deep learning to statistical and quantum many-body physics. 

## Eun-Ah Kim

Synergy between theory, experiment, and Machine Learning, covering: Topological Phases, Out of equilibrium phases, STM and X-ray data of charge order. Feedforward fully connected neural networks for supervised learning and gaussian mixture model for unsupervised learning. 

## Roger Melko

Lattice models for statistical physics, Monte Carlo methods, supervised and unsupervised learning, neural networks, Boltzmann machines, and deep learning. 

## Zohar Ringel

New results in ML: Mapping over-parameterized fixed-depth deep neural networks to Gaussian Processes. The Neural Tangent Kernel. Field theory description of over-parametrized networks. The information bottleneck. Identifying slow degrees of freedom, learning Exact Holomorphic Mappings in ADS/CFT.
